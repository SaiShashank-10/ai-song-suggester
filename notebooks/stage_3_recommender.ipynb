{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05dcbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from scikit-learn) (2.3.3)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached scipy-1.16.2-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ---------------------------------------- 4/4 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8c4d82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎵 Loading the Spotify song dataset...\n",
      "✅ Song dataset prepared with 5 normalized features!\n",
      "\n",
      "🤖 Creating 20 mood clusters with K-Means...\n",
      "✅ Clustering complete!\n",
      "💾 Clustered dataset saved to 'spotify_dataset_with_clusters.csv'\n",
      "💾 Scaler and K-Means model (with correct data type) saved to files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import joblib\n",
    "\n",
    "# --- 1. LOAD AND PREPARE THE DATASET ---\n",
    "print(\"🎵 Loading the Spotify song dataset...\")\n",
    "dataset_path = 'spotify_dataset.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "features_to_use = ['valence', 'energy', 'danceability', 'tempo', 'acousticness']\n",
    "df.dropna(subset=features_to_use, inplace=True)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df[features_to_use]), columns=features_to_use)\n",
    "\n",
    "# <-- THE FIRST FIX: Convert the entire dataset to float32 before training\n",
    "df_normalized = df_normalized.astype(np.float32)\n",
    "\n",
    "print(\"✅ Song dataset prepared with 5 normalized features!\")\n",
    "\n",
    "# --- 2. CREATE MOOD CLUSTERS USING K-MEANS ---\n",
    "n_clusters = 20\n",
    "print(f\"\\n🤖 Creating {n_clusters} mood clusters with K-Means...\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "# Assign a cluster label to each song\n",
    "df['cluster'] = kmeans.fit_predict(df_normalized)\n",
    "\n",
    "print(\"✅ Clustering complete!\")\n",
    "\n",
    "# --- 3. SAVE THE RESULTS ---\n",
    "clustered_dataset_path = 'spotify_dataset_with_clusters.csv'\n",
    "df.to_csv(clustered_dataset_path, index=False)\n",
    "print(f\"💾 Clustered dataset saved to '{clustered_dataset_path}'\")\n",
    "\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(kmeans, 'kmeans.pkl')\n",
    "print(\"💾 Scaler and K-Means model (with correct data type) saved to files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c77f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading models and clustered dataset...\n",
      "✅ All models and data loaded successfully!\n",
      "\n",
      "🖼️ Analyzing image: shash.jpeg\n",
      "✨ Image mapped to Mood Cluster #9\n",
      "\n",
      "✨ Top 5 Song Recommendations from the Cluster:\n",
      "                track_name                   artists  valence  energy  \\\n",
      "94842        see you leave  sorrow;Thomas Reid;Zaini    0.619   0.397   \n",
      "108836    Jamás Retornarás    Miguel Calo;Raúl Berón    0.513   0.304   \n",
      "93474   Парижские фантазии              Oleg Pogudin    0.660   0.339   \n",
      "45036   Prisionero Del Mar             Los Tecolines    0.812   0.283   \n",
      "105458        Mr Lightfoot            Riverside Park    0.424   0.124   \n",
      "\n",
      "        danceability  cluster  \n",
      "94842          0.695        9  \n",
      "108836         0.700        9  \n",
      "93474          0.739        9  \n",
      "45036          0.711        9  \n",
      "105458         0.673        9  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Re-import libraries and load saved models ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- 1. LOAD ALL MODELS AND DATA ---\n",
    "print(\"🧠 Loading models and clustered dataset...\")\n",
    "image_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "kmeans = joblib.load('kmeans.pkl')\n",
    "df_clustered = pd.read_csv('spotify_dataset_with_clusters.csv')\n",
    "print(\"✅ All models and data loaded successfully!\")\n",
    "\n",
    "\n",
    "# --- 2. IMAGE ANALYSIS FUNCTION ---\n",
    "def extract_image_features(img_path):\n",
    "    img = Image.open(img_path).resize((224, 224))\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = image_model.predict(x, verbose=0)\n",
    "    return features.flatten()\n",
    "\n",
    "\n",
    "# --- 3. FINAL RECOMMENDATION ENGINE ---\n",
    "def recommend_songs(image_path, top_n=5):\n",
    "    print(f\"\\n🖼️ Analyzing image: {image_path}\")\n",
    "    image_features = extract_image_features(image_path)\n",
    "    \n",
    "    raw_mood = np.array([[\n",
    "        np.mean(image_features), np.median(image_features), np.std(image_features),\n",
    "        np.max(image_features) - np.min(image_features), np.quantile(image_features, 0.75)\n",
    "    ]])\n",
    "    \n",
    "    scaled_mood = scaler.transform(raw_mood)\n",
    "    \n",
    "    # <-- THE SECOND FIX: Ensure input is also float32 for safety\n",
    "    scaled_mood = scaled_mood.astype(np.float32)\n",
    "    \n",
    "    predicted_cluster = kmeans.predict(scaled_mood)[0]\n",
    "    print(f\"✨ Image mapped to Mood Cluster #{predicted_cluster}\")\n",
    "    \n",
    "    cluster_songs = df_clustered[df_clustered['cluster'] == predicted_cluster]\n",
    "    \n",
    "    return cluster_songs.sample(n=min(top_n, len(cluster_songs)))\n",
    "\n",
    "\n",
    "# --- 4. HOW TO USE IT ---\n",
    "test_image_path = 'shash.jpeg' # Change this to your test image\n",
    "\n",
    "if os.path.exists(test_image_path):\n",
    "    recommended_songs = recommend_songs(test_image_path)\n",
    "    print(\"\\n✨ Top 5 Song Recommendations from the Cluster:\")\n",
    "    display_cols = ['track_name', 'artists', 'valence', 'energy', 'danceability', 'cluster']\n",
    "    print(recommended_songs[display_cols])\n",
    "else:\n",
    "    print(f\"\\n❌ ERROR: Test image '{test_image_path}' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b436125",
   "metadata": {},
   "source": [
    "FEATURE 2 - FEEDBACK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a6e46f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading models and clustered dataset...\n",
      "✅ All models and data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Re-import libraries and load saved models ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- 1. LOAD ALL MODELS AND DATA ---\n",
    "print(\"🧠 Loading models and clustered dataset...\")\n",
    "image_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "kmeans = joblib.load('kmeans.pkl')\n",
    "df_clustered = pd.read_csv('spotify_dataset_with_clusters.csv')\n",
    "print(\"✅ All models and data loaded successfully!\")\n",
    "\n",
    "# --- 2. SETUP FEEDBACK DATABASE ---\n",
    "FEEDBACK_FILE = 'user_feedback.csv'\n",
    "\n",
    "def save_feedback(user_id, track_id, feedback):\n",
    "    \"\"\"Saves user feedback to a CSV file.\"\"\"\n",
    "    new_feedback = pd.DataFrame([{'user_id': user_id, 'track_id': track_id, 'feedback': feedback}])\n",
    "    if not os.path.exists(FEEDBACK_FILE):\n",
    "        new_feedback.to_csv(FEEDBACK_FILE, index=False)\n",
    "    else:\n",
    "        new_feedback.to_csv(FEEDBACK_FILE, mode='a', header=False, index=False)\n",
    "    print(f\"👍 Feedback saved: User {user_id} {'liked' if feedback == 1 else 'disliked'} track {track_id}\")\n",
    "\n",
    "def get_user_likes(user_id):\n",
    "    \"\"\"Gets a list of all track IDs a user has liked.\"\"\"\n",
    "    if not os.path.exists(FEEDBACK_FILE):\n",
    "        return []\n",
    "    feedback_df = pd.read_csv(FEEDBACK_FILE)\n",
    "    user_feedback = feedback_df[feedback_df['user_id'] == user_id]\n",
    "    liked_tracks = user_feedback[user_feedback['feedback'] == 1]['track_id'].tolist()\n",
    "    return liked_tracks\n",
    "\n",
    "# --- 3. IMAGE ANALYSIS FUNCTION ---\n",
    "def extract_image_features(img_path):\n",
    "    img = Image.open(img_path).resize((224, 224))\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = image_model.predict(x, verbose=0)\n",
    "    return features.flatten()\n",
    "\n",
    "# --- 4. RECOMMENDATION ENGINE WITH FEEDBACK ---\n",
    "def recommend_songs(image_path, user_id, top_n=5):\n",
    "    print(f\"\\n🖼️ Analyzing image for User #{user_id}: {image_path}\")\n",
    "    image_features = extract_image_features(image_path)\n",
    "    \n",
    "    raw_mood = np.array([[\n",
    "        np.mean(image_features), np.median(image_features), np.std(image_features),\n",
    "        np.max(image_features) - np.min(image_features), np.quantile(image_features, 0.75)\n",
    "    ]])\n",
    "    scaled_mood = scaler.transform(raw_mood).astype(np.float32)\n",
    "    \n",
    "    predicted_cluster = kmeans.predict(scaled_mood)[0]\n",
    "    print(f\"✨ Image mapped to Mood Cluster #{predicted_cluster}\")\n",
    "    \n",
    "    # Get all songs from the predicted cluster\n",
    "    cluster_songs = df_clustered[df_clustered['cluster'] == predicted_cluster]\n",
    "    \n",
    "    # --- PERSONALIZATION LOGIC ---\n",
    "    # Get the list of songs this user has previously liked\n",
    "    user_liked_songs = get_user_likes(user_id)\n",
    "    \n",
    "    # Separate the cluster songs into \"liked\" and \"not liked\"\n",
    "    liked_in_cluster = cluster_songs[cluster_songs['track_id'].isin(user_liked_songs)]\n",
    "    other_songs = cluster_songs[~cluster_songs['track_id'].isin(user_liked_songs)]\n",
    "    \n",
    "    # Combine them, putting the liked songs first, then fill with other random songs\n",
    "    final_recommendations = pd.concat([liked_in_cluster, other_songs.sample(frac=1)]).head(top_n)\n",
    "    \n",
    "    return final_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64abc9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FIRST RECOMMENDATION (NO FEEDBACK YET) ---\n",
      "\n",
      "🖼️ Analyzing image for User #1: mountain.jpeg\n",
      "✨ Image mapped to Mood Cluster #19\n",
      "\n",
      "✨ Top 5 Song Recommendations:\n",
      "                      track_id  \\\n",
      "79200   1Ic5yiebfz4GfOTfJnzZ3W   \n",
      "28706   3eWjl48K6Kgn5Vr7iKWsh9   \n",
      "102351  32YHCNItnCdiYaPbw0uTo8   \n",
      "102585  4FN82sgnqneYuQnW9VFygo   \n",
      "73809   0S0zgiheqNBkRjEMo7pnig   \n",
      "\n",
      "                                               track_name  \\\n",
      "79200   I Know You - From The \"Fifty Shades Of Grey\" S...   \n",
      "28706                                           Right Now   \n",
      "102351  I Put A Spell On You (Fifty Shades of Grey) - ...   \n",
      "102585                        Won't Be Home For Christmas   \n",
      "73809                               Lightning Over Heaven   \n",
      "\n",
      "                                     artists  \n",
      "79200                            Skylar Grey  \n",
      "28706                           NURKO;Misdom  \n",
      "102351                          Annie Lennox  \n",
      "102585  Hootie & The Blowfish;Abigail Hodges  \n",
      "73809                      Amelie Lens;Anyma  \n",
      "👍 Feedback saved: User 1 liked track 3eWjl48K6Kgn5Vr7iKWsh9\n",
      "\n",
      "\n",
      "--- SECOND RECOMMENDATION (AFTER LIKING A SONG) ---\n",
      "\n",
      "🖼️ Analyzing image for User #1: mountain.jpeg\n",
      "✨ Image mapped to Mood Cluster #19\n",
      "\n",
      "✨ Top 5 Song Recommendations (Personalized):\n",
      "                     track_id  \\\n",
      "28706  3eWjl48K6Kgn5Vr7iKWsh9   \n",
      "29558  3eWjl48K6Kgn5Vr7iKWsh9   \n",
      "79200  1Ic5yiebfz4GfOTfJnzZ3W   \n",
      "70985  7dsU5xLqVQn4nrqg1TtHWF   \n",
      "54856  1HgMMME0o7usgpIljGNiUS   \n",
      "\n",
      "                                              track_name       artists  \n",
      "28706                                          Right Now  NURKO;Misdom  \n",
      "29558                                          Right Now  NURKO;Misdom  \n",
      "79200  I Know You - From The \"Fifty Shades Of Grey\" S...   Skylar Grey  \n",
      "70985                                                 也罷           阿吉仔  \n",
      "54856                                        Halley Wars        Seekae  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- 5. SIMULATION & USAGE ---\n",
    "# Define a test user and image\n",
    "test_user_id = 1\n",
    "test_image_path = 'mountain.jpeg' # Use an image in your /notebooks folder\n",
    "\n",
    "# 1. Get initial recommendations for the user\n",
    "print(\"--- FIRST RECOMMENDATION (NO FEEDBACK YET) ---\")\n",
    "initial_recs = recommend_songs(test_image_path, user_id=test_user_id)\n",
    "print(\"\\n✨ Top 5 Song Recommendations:\")\n",
    "display_cols = ['track_id', 'track_name', 'artists']\n",
    "print(initial_recs[display_cols])\n",
    "\n",
    "# 2. Simulate the user \"liking\" the second recommended song\n",
    "if not initial_recs.empty:\n",
    "    song_to_like = initial_recs.iloc[1] # Let's say they like the second song\n",
    "    save_feedback(user_id=test_user_id, track_id=song_to_like['track_id'], feedback=1) # 1 for \"like\"\n",
    "else:\n",
    "    print(\"\\nNo recommendations were generated to provide feedback on.\")\n",
    "\n",
    "# 3. Get recommendations for the SAME image AGAIN\n",
    "print(\"\\n\\n--- SECOND RECOMMENDATION (AFTER LIKING A SONG) ---\")\n",
    "new_recs = recommend_songs(test_image_path, user_id=test_user_id)\n",
    "print(\"\\n✨ Top 5 Song Recommendations (Personalized):\")\n",
    "print(new_recs[display_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb74285",
   "metadata": {},
   "source": [
    "VIDEO ANALYSIS - FEATURE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd3b4475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Using cached numpy-2.2.6-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "   ---------------------------------------- 0.0/39.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.9/39.0 MB 14.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 6.0/39.0 MB 14.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 8.9/39.0 MB 14.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 11.8/39.0 MB 14.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 14.7/39.0 MB 14.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 17.8/39.0 MB 14.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 19.4/39.0 MB 13.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 20.2/39.0 MB 12.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 23.3/39.0 MB 12.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 26.2/39.0 MB 13.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.4/39.0 MB 13.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 32.0/39.0 MB 13.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.4/39.0 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/39.0 MB 13.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.0/39.0 MB 13.3 MB/s  0:00:02\n",
      "Using cached numpy-2.2.6-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "Installing collected packages: numpy, opencv-python\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 2.3.3\n",
      "\n",
      "    Uninstalling numpy-2.3.3:\n",
      "\n",
      "      Successfully uninstalled numpy-2.3.3\n",
      "\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   -------------------- ------------------- 1/2 [opencv-python]\n",
      "   -------------------- ------------------- 1/2 [opencv-python]\n",
      "   -------------------- ------------------- 1/2 [opencv-python]\n",
      "   ---------------------------------------- 2/2 [opencv-python]\n",
      "\n",
      "Successfully installed numpy-2.2.6 opencv-python-4.12.0.88\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c581b6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading all models and the clustered dataset...\n",
      "✅ All models and data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import joblib\n",
    "import os\n",
    "import cv2  # The OpenCV library for video processing\n",
    "\n",
    "# --- 1. LOAD ALL MODELS AND DATA ---\n",
    "print(\"🧠 Loading all models and the clustered dataset...\")\n",
    "image_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "kmeans = joblib.load('kmeans.pkl')\n",
    "df_clustered = pd.read_csv('spotify_dataset_with_clusters.csv')\n",
    "print(\"✅ All models and data loaded successfully!\")\n",
    "\n",
    "# --- 2. FEEDBACK SYSTEM FUNCTIONS ---\n",
    "FEEDBACK_FILE = 'user_feedback.csv'\n",
    "def save_feedback(user_id, track_id, feedback):\n",
    "    new_feedback = pd.DataFrame([{'user_id': user_id, 'track_id': track_id, 'feedback': feedback}])\n",
    "    if not os.path.exists(FEEDBACK_FILE):\n",
    "        new_feedback.to_csv(FEEDBACK_FILE, index=False)\n",
    "    else:\n",
    "        new_feedback.to_csv(FEEDBACK_FILE, mode='a', header=False, index=False)\n",
    "    print(f\"👍 Feedback saved for User {user_id}\")\n",
    "\n",
    "def get_user_likes(user_id):\n",
    "    if not os.path.exists(FEEDBACK_FILE): return []\n",
    "    feedback_df = pd.read_csv(FEEDBACK_FILE)\n",
    "    user_likes = feedback_df[(feedback_df['user_id'] == user_id) & (feedback_df['feedback'] == 1)]\n",
    "    return user_likes['track_id'].tolist()\n",
    "\n",
    "# --- 3. MEDIA PROCESSING FUNCTIONS ---\n",
    "def extract_image_features(img):\n",
    "    \"\"\"Takes a PIL Image object and returns its feature vector.\"\"\"\n",
    "    img = img.resize((224, 224))\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = image_model.predict(x, verbose=0)\n",
    "    return features.flatten()\n",
    "\n",
    "def extract_frames_from_video(video_path, num_frames=5):\n",
    "    \"\"\"Extracts a set number of evenly spaced frames from a video.\"\"\"\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames < num_frames:\n",
    "        print(\"⚠️ Video is too short; using all available frames.\")\n",
    "        num_frames = total_frames\n",
    "    \n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    \n",
    "    for i in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Convert frame from OpenCV's BGR format to PIL's RGB format\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(frame_rgb))\n",
    "            \n",
    "    cap.release()\n",
    "    print(f\"📹 Extracted {len(frames)} frames from video.\")\n",
    "    return frames\n",
    "\n",
    "# --- 4. THE FINAL RECOMMENDATION ENGINE ---\n",
    "def get_recommendations(file_path, user_id, top_n=5):\n",
    "    print(f\"\\n▶️ Analyzing file for User #{user_id}: {file_path}\")\n",
    "    \n",
    "    # Determine if the file is an image or video\n",
    "    video_extensions = ['.mp4', '.mov', '.avi', '.mkv']\n",
    "    file_ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    # --- Feature Extraction ---\n",
    "    if file_ext in video_extensions:\n",
    "        # It's a video: extract frames and average their features\n",
    "        frames = extract_frames_from_video(file_path)\n",
    "        if not frames:\n",
    "            print(\"❌ Could not extract frames from video.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        frame_features = [extract_image_features(frame) for frame in frames]\n",
    "        final_features = np.mean(frame_features, axis=0)\n",
    "    else:\n",
    "        # It's an image: extract features directly\n",
    "        img = Image.open(file_path)\n",
    "        final_features = extract_image_features(img)\n",
    "        \n",
    "    # --- Mood Prediction & Recommendation (same as before) ---\n",
    "    raw_mood = np.array([[\n",
    "        np.mean(final_features), np.median(final_features), np.std(final_features),\n",
    "        np.max(final_features) - np.min(final_features), np.quantile(final_features, 0.75)\n",
    "    ]])\n",
    "    scaled_mood = scaler.transform(raw_mood).astype(np.float32)\n",
    "    predicted_cluster = kmeans.predict(scaled_mood)[0]\n",
    "    print(f\"✨ File mapped to Mood Cluster #{predicted_cluster}\")\n",
    "    \n",
    "    cluster_songs = df_clustered[df_clustered['cluster'] == predicted_cluster]\n",
    "    \n",
    "    # --- Personalization ---\n",
    "    user_liked_songs = get_user_likes(user_id)\n",
    "    liked_in_cluster = cluster_songs[cluster_songs['track_id'].isin(user_liked_songs)]\n",
    "    other_songs = cluster_songs[~cluster_songs['track_id'].isin(user_liked_songs)]\n",
    "    final_recommendations = pd.concat([liked_in_cluster, other_songs.sample(frac=1)]).head(top_n)\n",
    "    \n",
    "    return final_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "023a828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶️ Analyzing file for User #1: grass.jpeg\n",
      "✨ File mapped to Mood Cluster #19\n",
      "\n",
      "✨ Top 5 Recommendations for the Image:\n",
      "                                              track_name  \\\n",
      "28706                                          Right Now   \n",
      "29558                                          Right Now   \n",
      "79200  I Know You - From The \"Fifty Shades Of Grey\" S...   \n",
      "99134                                     Christmas Time   \n",
      "40504                                Teu Toque - Ao Vivo   \n",
      "\n",
      "                         artists  cluster  \n",
      "28706               NURKO;Misdom       19  \n",
      "29558               NURKO;Misdom       19  \n",
      "79200                Skylar Grey       19  \n",
      "99134                Bryan Adams       19  \n",
      "40504  Gabi Sampaio;Nívea Soares       19  \n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "▶️ Analyzing file for User #1: trek.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📹 Extracted 5 frames from video.\n",
      "✨ File mapped to Mood Cluster #19\n",
      "\n",
      "✨ Top 5 Recommendations for the Video:\n",
      "                                              track_name        artists  \\\n",
      "28706                                          Right Now   NURKO;Misdom   \n",
      "29558                                          Right Now   NURKO;Misdom   \n",
      "79200  I Know You - From The \"Fifty Shades Of Grey\" S...    Skylar Grey   \n",
      "40834                                 Esperar É Caminhar   Palavrantiga   \n",
      "62512                                            ひまわりの約束  Motohiro Hata   \n",
      "\n",
      "       cluster  \n",
      "28706       19  \n",
      "29558       19  \n",
      "79200       19  \n",
      "40834       19  \n",
      "62512       19  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- 5. USAGE EXAMPLE ---\n",
    "test_user_id = 1\n",
    "\n",
    "# --- Test with an Image ---\n",
    "test_image_path = 'grass.jpeg' # Make sure you have an image file\n",
    "if os.path.exists(test_image_path):\n",
    "    image_recs = get_recommendations(test_image_path, user_id=test_user_id)\n",
    "    print(\"\\n✨ Top 5 Recommendations for the Image:\")\n",
    "    print(image_recs[['track_name', 'artists', 'cluster']])\n",
    "else:\n",
    "    print(f\"⚠️ Test image '{test_image_path}' not found.\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\") # Separator\n",
    "\n",
    "# --- Test with a Video ---\n",
    "test_video_path = 'trek.mp4' # Add a short video file to your folder\n",
    "if os.path.exists(test_video_path):\n",
    "    video_recs = get_recommendations(test_video_path, user_id=test_user_id)\n",
    "    print(\"\\n✨ Top 5 Recommendations for the Video:\")\n",
    "    print(video_recs[['track_name', 'artists', 'cluster']])\n",
    "else:\n",
    "    print(f\"⚠️ Test video '{test_video_path}' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4944c1d0",
   "metadata": {},
   "source": [
    "OPTION 1 - USING HUGGING FACE MODAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a7a5fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.8.0-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.23.0-cp313-cp313-win_amd64.whl.metadata (6.1 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.9.18-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\shashank\\ai-song-suggester\\.venv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Using cached transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "   ---------------------------------------- 0.0/563.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 563.3/563.3 kB 14.2 MB/s  0:00:00\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Using cached torch-2.8.0-cp313-cp313-win_amd64.whl (241.3 MB)\n",
      "Downloading torchvision-0.23.0-cp313-cp313-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 8.3 MB/s  0:00:00\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached regex-2025.9.18-cp313-cp313-win_amd64.whl (275 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: mpmath, sympy, safetensors, regex, networkx, fsspec, filelock, torch, huggingface-hub, torchvision, tokenizers, transformers\n",
      "\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [safetensors]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ---------------- -----------------------  5/12 [fsspec]\n",
      "   ---------------- -----------------------  5/12 [fsspec]\n",
      "   ---------------- -----------------------  5/12 [fsspec]\n",
      "   -------------------- -------------------  6/12 [filelock]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   ----------------------- ----------------  7/12 [torch]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   -------------------------- -------------  8/12 [huggingface-hub]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   ------------------------------ ---------  9/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [tokenizers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ------------------------------------ --- 11/12 [transformers]\n",
      "   ---------------------------------------- 12/12 [transformers]\n",
      "\n",
      "Successfully installed filelock-3.19.1 fsspec-2025.9.0 huggingface-hub-0.35.1 mpmath-1.3.0 networkx-3.5 regex-2025.9.18 safetensors-0.6.2 sympy-1.14.0 tokenizers-0.22.1 torch-2.8.0 torchvision-0.23.0 transformers-4.56.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "877474d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Initializing session...\n",
      "✅ All models and data are ready!\n",
      "\n",
      "🖼️ Analyzing image: mountain.jpeg\n",
      "✨ Detected Tags: ['valley, vale', 'worm fence, snake fence, snake-rail fence, Virginia fence', 'lakeside, lakeshore', 'alp', 'hay']\n",
      "✨ Image mapped to Mood Cluster #11\n",
      "\n",
      "✨ Top 5 'Smarter' Song Recommendations:\n",
      "          track_name             artists  valence  energy  danceability  \\\n",
      "21464   Gvnman Shift               Skeng    0.368   0.570         0.911   \n",
      "7801         Get Sad  Greensky Bluegrass    0.488   0.400         0.531   \n",
      "62417          Lemon       Kenshi Yonezu    0.446   0.661         0.532   \n",
      "112657    ROADRUNNER    Jefe;Bar B;Bekom    0.441   0.464         0.809   \n",
      "3383      Automobile               KALEO    0.356   0.553         0.692   \n",
      "\n",
      "        cluster  \n",
      "21464        11  \n",
      "7801         11  \n",
      "62417        11  \n",
      "112657       11  \n",
      "3383         11  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import warnings\n",
    "\n",
    "# Suppress verbose warnings from scikit-learn\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- 1. LOAD DATA AND BUILD MODELS ON-THE-FLY ---\n",
    "print(\"🧠 Initializing session...\")\n",
    "\n",
    "# Load the base dataset\n",
    "df_clustered = pd.read_csv('spotify_dataset_with_clusters.csv')\n",
    "\n",
    "# Prepare the features for clustering (using float64, the default)\n",
    "features_to_use = ['valence', 'energy', 'danceability', 'tempo', 'acousticness']\n",
    "df_clustered.dropna(subset=features_to_use, inplace=True)\n",
    "scaler = MinMaxScaler()\n",
    "song_features_normalized = scaler.fit_transform(df_clustered[features_to_use])\n",
    "\n",
    "# Train the K-Means model in memory\n",
    "n_clusters = 20\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "df_clustered['cluster'] = kmeans.fit_predict(song_features_normalized)\n",
    "\n",
    "# Load the image tagging model\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "tagging_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "print(\"✅ All models and data are ready!\")\n",
    "\n",
    "\n",
    "# --- 2. THE MOOD DICTIONARY ---\n",
    "mood_map = {\n",
    "    ('party', 'celebration', 'concert', 'crowd'): {'valence': 0.3, 'energy': 0.2, 'danceability': 0.3},\n",
    "    ('smile', 'joy', 'laughing'): {'valence': 0.4, 'energy': 0.1},\n",
    "    ('beach', 'seashore', 'coast'): {'valence': 0.2, 'energy': -0.2, 'acousticness': 0.1},\n",
    "    ('sunset', 'sunrise', 'landscape'): {'valence': 0.1, 'energy': -0.3, 'acousticness': 0.3},\n",
    "    ('forest', 'nature', 'mountain'): {'valence': 0.1, 'energy': -0.2, 'acousticness': 0.4},\n",
    "    ('sad', 'gloomy', 'rain'): {'valence': -0.4, 'energy': -0.3},\n",
    "    ('dark', 'night'): {'energy': -0.2},\n",
    "    ('car', 'driving', 'road'): {'energy': 0.2, 'tempo': 0.1},\n",
    "    ('sports', 'running'): {'energy': 0.3, 'tempo': 0.2},\n",
    "}\n",
    "\n",
    "# --- 3. HELPER & RECOMMENDATION FUNCTIONS ---\n",
    "def get_image_tags(img, top_k=5):\n",
    "    \"\"\"Uses the Vision Transformer to get descriptive tags for an image.\"\"\"\n",
    "    inputs = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "    outputs = tagging_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    top_indices = logits[0].topk(top_k).indices.tolist()\n",
    "    tags = [tagging_model.config.id2label[i] for i in top_indices]\n",
    "    return tags\n",
    "\n",
    "def get_recommendations_smarter(file_path, top_n=5):\n",
    "    print(f\"\\n🖼️ Analyzing image: {file_path}\")\n",
    "    img = Image.open(file_path)\n",
    "    \n",
    "    tags = get_image_tags(img)\n",
    "    print(f\"✨ Detected Tags: {tags}\")\n",
    "    \n",
    "    # Create the mood vector with the required float64 type\n",
    "    target_mood = np.array([0.5, 0.5, 0.5, 0.5, 0.5], dtype=np.float64)\n",
    "    \n",
    "    for tag in tags:\n",
    "        for keywords, mood_values in mood_map.items():\n",
    "            if any(keyword in tag for keyword in keywords):\n",
    "                target_mood[0] += mood_values.get('valence', 0.0)\n",
    "                target_mood[1] += mood_values.get('energy', 0.0)\n",
    "                target_mood[2] += mood_values.get('danceability', 0.0)\n",
    "                target_mood[3] += mood_values.get('tempo', 0.0)\n",
    "                target_mood[4] += mood_values.get('acousticness', 0.0)\n",
    "                \n",
    "    target_mood = np.clip(target_mood, 0, 1)\n",
    "    \n",
    "    # Scale the target mood using the scaler we created in this session\n",
    "    scaled_mood = scaler.transform(target_mood.reshape(1, -1))\n",
    "    \n",
    "    # Predict the cluster using the model we created in this session\n",
    "    predicted_cluster = kmeans.predict(scaled_mood)[0]\n",
    "    print(f\"✨ Image mapped to Mood Cluster #{predicted_cluster}\")\n",
    "    \n",
    "    cluster_songs = df_clustered[df_clustered['cluster'] == predicted_cluster]\n",
    "    return cluster_songs.sample(n=min(top_n, len(cluster_songs)))\n",
    "\n",
    "\n",
    "# --- 4. HOW TO USE IT ---\n",
    "test_image_path = 'mountain.jpeg' # Change to your image file\n",
    "\n",
    "if os.path.exists(test_image_path):\n",
    "    recommended_songs = get_recommendations_smarter(test_image_path)\n",
    "    print(\"\\n✨ Top 5 'Smarter' Song Recommendations:\")\n",
    "    display_cols = ['track_name', 'artists', 'valence', 'energy', 'danceability', 'cluster']\n",
    "    print(recommended_songs[display_cols])\n",
    "else:\n",
    "    print(f\"\\n❌ ERROR: Test image '{test_image_path}' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6429ac",
   "metadata": {},
   "source": [
    "BOTH IMAGE AND VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e790365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Initializing session...\n",
      "✅ All models and data are ready!\n",
      "\n",
      "▶️ Analyzing file: grass.jpeg\n",
      "✨ Detected Tags: ['rapeseed', 'balloon', 'hay', 'flagpole, flagstaff', 'pole']\n",
      "✨ Media mapped to Mood Cluster #11\n",
      "\n",
      "✨ Top 5 Recommendations for the IMAGE:\n",
      "            track_name        artists  valence  energy  danceability  cluster\n",
      "94826        about you  sadeyes;Powfu    0.390   0.496         0.566       11\n",
      "84763          Şakalar          Flört    0.503   0.481         0.638       11\n",
      "24013              Aja     Jay Daniel    0.225   0.464         0.790       11\n",
      "67513          Algodón          Lasso    0.444   0.552         0.741       11\n",
      "99620  Stop This Train     John Mayer    0.408   0.437         0.619       11\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "▶️ Analyzing file: trek.mp4\n",
      "📹 Extracted 5 frames from video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ Detected Tags: ['mountain bike, all-terrain bike, off-roader', 'lakeside, lakeshore', 'valley, vale', 'maze, labyrinth', 'greenhouse, nursery, glasshouse', 'cliff, drop, drop-off', 'balloon', 'hay', 'volcano']\n",
      "✨ Media mapped to Mood Cluster #6\n",
      "\n",
      "✨ Top 5 Recommendations for the VIDEO:\n",
      "                            track_name                  artists  valence  \\\n",
      "79882                    Zero O' Clock             Smyang Piano    0.165   \n",
      "15549                    Healing Touch                DaniSogen    0.100   \n",
      "15229             i still think of you  the bootleg boy;Maberry    0.308   \n",
      "41093  Det tog så lång tid att bli ung          Håkan Hellström    0.362   \n",
      "45042             Qué Manera de Perder             Cuco Sánchez    0.257   \n",
      "\n",
      "       energy  danceability  cluster  \n",
      "79882  0.0727         0.469        6  \n",
      "15549  0.2250         0.670        6  \n",
      "15229  0.1930         0.575        6  \n",
      "41093  0.2970         0.405        6  \n",
      "45042  0.0909         0.490        6  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Shashank\\AI-Song-Suggester\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import warnings\n",
    "import cv2 # <-- NEW: Import OpenCV for video processing\n",
    "\n",
    "# Suppress verbose warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- 1. LOAD DATA AND BUILD MODELS ON-THE-FLY ---\n",
    "print(\"🧠 Initializing session...\")\n",
    "df_clustered = pd.read_csv('spotify_dataset_with_clusters.csv')\n",
    "features_to_use = ['valence', 'energy', 'danceability', 'tempo', 'acousticness']\n",
    "df_clustered.dropna(subset=features_to_use, inplace=True)\n",
    "scaler = MinMaxScaler()\n",
    "song_features_normalized = scaler.fit_transform(df_clustered[features_to_use])\n",
    "n_clusters = 20\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "df_clustered['cluster'] = kmeans.fit_predict(song_features_normalized)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "tagging_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "print(\"✅ All models and data are ready!\")\n",
    "\n",
    "\n",
    "# --- 2. THE MOOD DICTIONARY ---\n",
    "mood_map = {\n",
    "    ('party', 'celebration', 'concert', 'crowd'): {'valence': 0.3, 'energy': 0.2, 'danceability': 0.3},\n",
    "    ('smile', 'joy', 'laughing'): {'valence': 0.4, 'energy': 0.1},\n",
    "    ('beach', 'seashore', 'coast'): {'valence': 0.2, 'energy': -0.2, 'acousticness': 0.1},\n",
    "    ('sunset', 'sunrise', 'landscape'): {'valence': 0.1, 'energy': -0.3, 'acousticness': 0.3},\n",
    "    ('forest', 'nature', 'mountain'): {'valence': 0.1, 'energy': -0.2, 'acousticness': 0.4},\n",
    "    ('sad', 'gloomy', 'rain'): {'valence': -0.4, 'energy': -0.3},\n",
    "    ('dark', 'night'): {'energy': -0.2},\n",
    "    ('car', 'driving', 'road'): {'energy': 0.2, 'tempo': 0.1},\n",
    "    ('sports', 'running'): {'energy': 0.3, 'tempo': 0.2},\n",
    "}\n",
    "\n",
    "# --- 3. HELPER & RECOMMENDATION FUNCTIONS ---\n",
    "def get_image_tags(img, top_k=3): # Get fewer tags per frame to find the main subject\n",
    "    \"\"\"Uses the Vision Transformer to get descriptive tags for a single image (or frame).\"\"\"\n",
    "    inputs = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "    outputs = tagging_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    top_indices = logits[0].topk(top_k).indices.tolist()\n",
    "    tags = [tagging_model.config.id2label[i] for i in top_indices]\n",
    "    return tags\n",
    "\n",
    "def extract_frames_from_video(video_path, num_frames=5):\n",
    "    \"\"\"Extracts a set number of evenly spaced frames from a video.\"\"\"\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"❌ Error opening video file\")\n",
    "        return frames\n",
    "\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    \n",
    "    for i in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Convert frame from OpenCV's BGR format to PIL's RGB format\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(frame_rgb))\n",
    "            \n",
    "    cap.release()\n",
    "    print(f\"📹 Extracted {len(frames)} frames from video.\")\n",
    "    return frames\n",
    "\n",
    "def get_recommendations(file_path, top_n=5):\n",
    "    print(f\"\\n▶️ Analyzing file: {file_path}\")\n",
    "    \n",
    "    # --- Step A: Get tags from Image or Video ---\n",
    "    video_extensions = ['.mp4', '.mov', 'avi', 'mkv']\n",
    "    file_ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    all_tags = []\n",
    "    if file_ext in video_extensions:\n",
    "        frames = extract_frames_from_video(file_path)\n",
    "        if not frames:\n",
    "            print(\"Could not process video. Aborting recommendation.\")\n",
    "            return\n",
    "        for frame in frames:\n",
    "            all_tags.extend(get_image_tags(frame))\n",
    "    else: # Assume it's an image\n",
    "        img = Image.open(file_path)\n",
    "        all_tags = get_image_tags(img, top_k=5) # Get more tags for a single image\n",
    "\n",
    "    print(f\"✨ Detected Tags: {list(set(all_tags))}\") # Show unique tags\n",
    "    \n",
    "    # --- Step B: Calculate Mood (same logic as before) ---\n",
    "    target_mood = np.array([0.5, 0.5, 0.5, 0.5, 0.5], dtype=np.float64)\n",
    "    for tag in all_tags:\n",
    "        for keywords, mood_values in mood_map.items():\n",
    "            if any(keyword in tag for keyword in keywords):\n",
    "                target_mood[0] += mood_values.get('valence', 0.0)\n",
    "                target_mood[1] += mood_values.get('energy', 0.0)\n",
    "                target_mood[2] += mood_values.get('danceability', 0.0)\n",
    "                target_mood[3] += mood_values.get('tempo', 0.0)\n",
    "                target_mood[4] += mood_values.get('acousticness', 0.0)\n",
    "    \n",
    "    target_mood = np.clip(target_mood, 0, 1)\n",
    "    scaled_mood = scaler.transform(target_mood.reshape(1, -1))\n",
    "    \n",
    "    # --- Step C: Predict and Recommend (same logic as before) ---\n",
    "    predicted_cluster = kmeans.predict(scaled_mood)[0]\n",
    "    print(f\"✨ Media mapped to Mood Cluster #{predicted_cluster}\")\n",
    "    cluster_songs = df_clustered[df_clustered['cluster'] == predicted_cluster]\n",
    "    return cluster_songs.sample(n=min(top_n, len(cluster_songs)))\n",
    "\n",
    "# --- 4. HOW TO USE IT ---\n",
    "# Test with an image\n",
    "test_image_path = 'grass.jpeg' # Change to your image file\n",
    "if os.path.exists(test_image_path):\n",
    "    recommended_songs = get_recommendations(test_image_path)\n",
    "    print(\"\\n✨ Top 5 Recommendations for the IMAGE:\")\n",
    "    display_cols = ['track_name', 'artists', 'valence', 'energy', 'danceability', 'cluster']\n",
    "    print(recommended_songs[display_cols])\n",
    "else:\n",
    "    print(f\"\\n❌ ERROR: Test image '{test_image_path}' not found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\") # Separator\n",
    "\n",
    "# Test with a video\n",
    "test_video_path = 'trek.mp4' # Change to your video file\n",
    "if os.path.exists(test_video_path):\n",
    "    recommended_songs = get_recommendations(test_video_path)\n",
    "    print(\"\\n✨ Top 5 Recommendations for the VIDEO:\")\n",
    "    display_cols = ['track_name', 'artists', 'valence', 'energy', 'danceability', 'cluster']\n",
    "    print(recommended_songs[display_cols])\n",
    "else:\n",
    "    print(f\"\\n❌ ERROR: Test video '{test_video_path}' not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
